{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n"
     ]
    }
   ],
   "source": [
    "sentiment140_csv = pd.read_csv('../datasets/sentiment140/sentiment140.txt', delimiter=',', encoding=\"latin-1\", header=None, names=['label', 'd1','d2','d3', 'd4', 'tweet'])\n",
    "\n",
    "print(len(sentiment140_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label          d1                            d2        d3               d4  \\\n",
       "0      0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1      0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2      0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3      0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4      0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                               tweet  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment140_df = sentiment140_csv.drop(['d1','d2','d3','d4'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label meaning\n",
    "Positive (4), negative (0) or neutral (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1      0  is upset that he can't update his Facebook by ...\n",
       "2      0  @Kenichan I dived many times for the ball. Man...\n",
       "3      0    my whole body feels itchy and like its on fire \n",
       "4      0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Preprocess the texts:\n",
    "- Convert all characters to lower case\n",
    "- Remove special characters: Remove links and usernames and trasform emojis to text\n",
    "- Remove repetitions: Remove char repetitions (e.g. whaaaaat => what)\n",
    "- Remove stop words: Remove common stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Araceli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from time import time\n",
    "import nltk\n",
    "from emoji import demojize\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up: 462.24 sec\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-de9ef36ee2b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time to clean up: {:.2f} sec'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "texts = sentiment140_df['tweet']\n",
    "\n",
    "start = time()\n",
    "\n",
    "# Lowercasing\n",
    "texts = texts.str.lower()\n",
    "\n",
    "# Remove special chars\n",
    "texts = texts.str.replace(r\"(http|@)\\S+\", \"\")\n",
    "texts = texts.apply(demojize)\n",
    "texts = texts.str.replace(r\"::\", \": :\")\n",
    "texts = texts.str.replace(r\"â€™\", \"'\")\n",
    "texts = texts.str.replace(r\"[^a-z\\':_]\", \" \")\n",
    "\n",
    "#Remove repetitions\n",
    "pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "texts = texts.str.replace(pattern, r\"\\1\")\n",
    "\n",
    "\n",
    "# Transform short negation form\n",
    "texts = texts.str.replace(r\"(can't|cannot)\", 'can not')\n",
    "texts = texts.str.replace(r\"n't\", ' not')\n",
    "\n",
    "# Remove stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.remove('not')\n",
    "stopwords.remove('nor')\n",
    "stopwords.remove('no')\n",
    "texts = texts.apply(\n",
    "    lambda x: ' '.join([word for word in x.split() if word not in stopwords])\n",
    ")\n",
    "\n",
    "print('Time to clean up: {:.2f} sec'.format(time() - start))\n",
    "\n",
    "sentiment140_df.tweet = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>aw that's bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset not update facebook texting might cry re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dived many times ball managed save rest go bounds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feels itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>no not behaving i'm mad not see</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      0  aw that's bummer shoulda got david carr third day\n",
       "1      0  upset not update facebook texting might cry re...\n",
       "2      0  dived many times ball managed save rest go bounds\n",
       "3      0                   whole body feels itchy like fire\n",
       "4      0                    no not behaving i'm mad not see"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment140_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "Transform the text corpus to a vector representation\n",
    "\n",
    "- **num_words**: Number of words to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, lower=True)\n",
    "tokenizer.fit_on_texts(sentiment140_df.tweet)\n",
    "\n",
    "file_to_save = Path('../datasets/sentiment140/tokenizer.pickle').resolve()\n",
    "with file_to_save.open('wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Split the dataset in train and test data (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(columns=['label', 'tweet'])\n",
    "test = pd.DataFrame(columns=['label', 'tweet'])\n",
    "\n",
    "for label in sentiment140_df['label'].unique():\n",
    "    label_data = sentiment140_df[sentiment140_df['label'] == label]\n",
    "    train_data, test_data = train_test_split(label_data, test_size = 0.3)\n",
    "    \n",
    "    train = pd.concat([train, train_data])\n",
    "    test = pd.concat([test, test_data])\n",
    "    \n",
    "# TODO: Verify behavior with conventinoal train_test_split method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Define the bidirectional GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, GRU\n",
    "from tensorflow.keras.layers import Dropout, GlobalAvgPool1D\n",
    "from tensorflow.keras.layers import Bidirectional, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = min(tokenizer.num_words, len(tokenizer.word_index) + 1)\n",
    "embedding_dim = 200\n",
    "input_length = 100\n",
    "gru_units = 128\n",
    "gru_dropout = 0.1\n",
    "recurrent_dropout = 0.1\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimator\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=embedding_dim,\n",
    "    input_shape=(input_length,)\n",
    "))\n",
    "\n",
    "model.add(Bidirectional(GRU(\n",
    "    gru_units,\n",
    "    return_sequences=True,\n",
    "    dropout=gru_dropout,\n",
    "    recurrent_dropout=recurrent_dropout\n",
    ")))\n",
    "\n",
    "model.add(GlobalAvgPool1D())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Araceli\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 200)          2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 256)          252672    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,260,929\n",
      "Trainable params: 2,260,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "Prepare the model input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [text.split() for text in train['tweet']]\n",
    "test_sequences = [text.split() for text in test['tweet']]\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train_sequences)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(test_sequences)\n",
    "\n",
    "x_train = pad_sequences(list_tokenized_train, maxlen=input_length)\n",
    "x_test = pad_sequences(list_tokenized_test, maxlen=input_length)\n",
    "\n",
    "y_train = train['label'].replace(4, 1)\n",
    "y_test = test['label'].replace(4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Training model with previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1120000 samples, validate on 480000 samples\n",
      "1120000/1120000 [==============================] - 14922s 13ms/sample - loss: 0.4486 - acc: 0.7896 - val_loss: 0.4198 - val_acc: 0.8055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1776d491dc8>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_train, \n",
    "    y=y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = Path('../models/sentiment_analysis/gru_model.h5').resolve()\n",
    "model.save_weights(model_file.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_test = [\"I feel sick and I dont want to go to the school, but tomorrow is going to be a better day\", \n",
    "             \"I feel sick and I dont want to go to the school\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "value_to_predict = \"This is awesome, not really\".split()\n",
    "list_tokenized_value_to_predict = tokenizer.texts_to_sequences(value_to_predict)\n",
    "\n",
    "x_prediction = pad_sequences(list_tokenized_value_to_predict, maxlen=input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., 4882, 5445,   43],\n",
       "       [   0,    0,    0, ...,    0,   96,  190],\n",
       "       [   0,    0,    0, ..., 6268, 6515,   90],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    6, 1541,   43],\n",
       "       [   0,    0,    0, ...,  205,    3,   15],\n",
       "       [   0,    0,    0, ...,  214,   18,  413]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Score Range: 0.608004 - 1.000000\n"
     ]
    }
   ],
   "source": [
    "# small testing\n",
    "\n",
    "import numpy as np\n",
    "def get_score_range(mean):\n",
    "  if mean < 0.5:\n",
    "    return (0.0, mean)\n",
    "  return (mean, 1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = model.predict(x_prediction)\n",
    "mean = np.mean(result)\n",
    "std = np.std(result)\n",
    "low, high = get_score_range(mean)\n",
    "print( \": Score Range: {:4f} - {:4f}\".format(low, high))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
